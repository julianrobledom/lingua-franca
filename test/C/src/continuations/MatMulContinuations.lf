/**
 * As a benchmark, this program has a few interesting properties:
 * - It hits the event queue hard by sending work items to hundreds of workers.
 *   Obviously a more optimized design would not do that unless it were running
 *   on hardware with hundreds of cores. But it might be worthwhile to handle
 *   this adverse condition better. Right now we have a linear-time operation
 *   (pqueue_find_equal_same_priority) taking 60% of instructions executed and
 *   about a third of clock cycles.
 * - Stack copying overhead seems pretty small.
 * - No under-the-table shared memory, unlike the Savina benchmark, and lots of
 *   data movement as a result. This is exacerbated by the number of workers,
 *   which hurts spatial locality.
 */

target C {Build-Type: Debug, logging: DEBUG}

import TrackSyncronizedRequest from "lib/TrackRequest.lf"

preamble {=
    #include <setjmp.h>
    #include "context_switch.h"
    #define N 1
    typedef int vector[N];
    typedef int matrix[N][N];   // row-major order
    LF_DECLARE_REQUEST(vector)
    LF_DECLARE_REQUEST(int)
=}

reactor MatMul(n: int(4)) {
    state ra: jmp_buf  // injected code

    state A: matrix
    state B: matrix
    state C: matrix  // TODO: Dynamically allocate C, and put the pointer to C on the stack?

    dot_products = new[n] DotProduct(n(n))
    row_distributor = new PassThroughVector()
    get_products = new TrackSyncronizedRequest(fanout(n))
    dot_products.product -> get_products.responses_in after 0

    (row_distributor.out)+ -> dot_products.row
    (get_products.request_out)+ -> dot_products.go

    reaction(startup) {=
        // No dynamic memory allocation required!
        for (int row = 0; row < self->n; row++) {
            for (int col = 0; col < self->n; col++) {
                self->A[row][col] = 2 * row + col;
                self->B[row][col] = row + 2 * col;
            }
        }
    =}

    reaction(startup, get_products.done) get_products.responses_out ->
    dot_products.column, row_distributor.in, get_products.request_in {=
        instant_t parallel_matmul_start_time = lf_time_physical();
        // Distribute columns of the matrix
        for (int row = 0; row < self->n; row++) {
            for (int col = 0; col < self->n; col++) {
                // You can't win here: Read locality or write locality, but not both.
                dot_products[col].column->value[row] = self->B[row][col];
            }
        }
        for (int col = 0; col < self->n; col++) {
            _lf_set_present((lf_port_base_t*) dot_products[col].column);
        }
        // Compute dot products in parallel
        for (int row = 0; row < self->n; row++) {
            for (int acol = 0; acol < self->n; acol++) {
                row_distributor.in->value[acol] = self->A[row][acol];
            }
            _lf_set_present((lf_port_base_t*) row_distributor.in);
            // Assume all products will come back at the same time
            lf_call(get_products.request_in, 0, get_products.done);
            for (int bcol = 0; bcol < self->n; bcol++) {
                self->C[row][bcol] = get_products.responses_out[bcol]->value;
            }
        }
        instant_t parallel_matmul_end_time = lf_time_physical();
        // Check that the result is correct
        for (int arow = 0; arow < self->n; arow++) {
            for (int bcol = 0; bcol < self->n; bcol++) {
                int expected = 0;
                for (int s = 0; s < self->n; s++) {
                    expected += self->A[arow][s] * self->B[s][bcol];
                }
                if (self->C[arow][bcol] != expected) {
                    lf_print_error_and_exit("Fail: %d != %d.", self->C[arow][bcol], expected);
                };
            }
        }
        instant_t checking_end_time = lf_time_physical();
        lf_print(
            "Time to compute in parallel: " PRINTF_TIME " milliseconds.",
            (parallel_matmul_end_time - parallel_matmul_start_time) / 1000000
        );
        // Note that checking does not involve memory writes, and so might be
        // faster than an actual naive matmul implementation
        lf_print(
            "Time to check results: " PRINTF_TIME " milliseconds.",
            (checking_end_time - parallel_matmul_end_time) / 1000000
        );
        lf_print("Success.");
    =}
}

reactor DotProduct(n: int(4)) {
    input column: vector
    input row: vector
    input go: request<int>
    output product: int

    state c: vector

    reaction(column) {=
        for (int i = 0; i < self->n; i++) {
            self->c[i] = column->value[i];
        }
    =}
    reaction(go) row -> product {=
        int sum = 0;
        for (int i = 0; i < self->n; i++) {
            sum += self->c[i] * row->value[i];
        }
        lf_set(product, sum);
    =}
}

reactor PassThroughVector {
    input in: vector
    output out: vector
    in -> out
}

reactor PassThroughInt {
    input in: int
    output out: int
    in -> out
}

main reactor {
    matmul = new MatMul(n(1))  // FIXME: you must update the preamble according to N
}
